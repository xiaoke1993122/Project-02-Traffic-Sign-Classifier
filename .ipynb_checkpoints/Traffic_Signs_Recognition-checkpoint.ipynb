{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "----\n",
    "Visualize the German Traffic Signs Dataset. This is open ended, some suggestions include: plotting traffic signs images, plotting the count of each sign, etc. Be creative!\n",
    "\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- features -> the images pixel values, (width, height, channels)\n",
    "- labels -> the label of the traffic sign\n",
    "- sizes -> the original width and height of the image, (width, height)\n",
    "- coords -> coordinates of a bounding box around the sign in the image, (x1, y1, x2, y2). Based the original image (not the resized version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test importing all required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "print('Modules loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "training_file = 'lab-2-data/train.p'\n",
    "testing_file = 'lab-2-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_img = len(X_train)\n",
    "num_height = len(X_train[0])\n",
    "num_width = len(X_train[0,0])\n",
    "num_colors = len(X_train[0,0,0])\n",
    "\n",
    "print('Images: ' + str(num_img) + '\\n' + \\\n",
    "      'Height: ' + str(num_height) + '\\n' + \\\n",
    "      'Width: ' + str(num_width) + '\\n' + \\\n",
    "      'Colors: ' + str(num_colors) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: what's the shape of an image?\n",
    "num_img = len(X_test)\n",
    "num_height = len(X_test[0])\n",
    "num_width = len(X_test[0][0])\n",
    "num_colors = len(X_test[0][0][0])\n",
    "\n",
    "print('Images: ' + str(num_img) + '\\n' + \\\n",
    "      'Height: ' + str(num_height) + '\\n' + \\\n",
    "      'Width: ' + str(num_width) + '\\n' + \\\n",
    "      'Colors: ' + str(num_colors) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### To start off let's do a basic data summary.\n",
    "\n",
    "# TODO: number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# TODO: number of testing examples\n",
    "n_test = len(X_test)\n",
    "\n",
    "# TODO: how many classes are in the dataset\n",
    "n_classes = 43\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "#print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "#print(X_train[0])\n",
    "\n",
    "\n",
    "\n",
    "print(X_train[0,0,0,:])\n",
    "\n",
    "\n",
    "# Show first and last image in database.\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(X_train[39208])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print number of each class\n",
    "\n",
    "#print( y_train )\n",
    "\n",
    "#print( len(np.bincount(y_train)) )\n",
    "\n",
    "#print( np.bincount(y_train) )\n",
    "\n",
    "\n",
    "signnameslist = []\n",
    "with open('signnames.csv') as csvfile:\n",
    "    signnames = csv.DictReader(csvfile)\n",
    "    for row in signnames:\n",
    "        signnameslist.append(row['SignName'])\n",
    "        \n",
    "#print(signnameslist)\n",
    "        \n",
    "y_pos = np.arange(len(signnameslist))\n",
    "number_sign = np.bincount(y_train)\n",
    "\n",
    "\n",
    "barhstart = 0\n",
    "barhend = 22\n",
    "plt.bar(y_pos[barhstart:barhend],number_sign[barhstart:barhend], alpha=0.4)\n",
    "plt.xticks(y_pos[barhstart:barhend], signnameslist[barhstart:barhend], rotation=45, horizontalalignment='right',size=6)\n",
    "plt.ylabel('Number of Pictures')\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "barhstart = 22\n",
    "barhend = 43\n",
    "plt.bar(y_pos[barhstart:barhend],number_sign[barhstart:barhend], alpha=0.4)\n",
    "plt.xticks(y_pos[barhstart:barhend], signnameslist[barhstart:barhend], rotation=45, horizontalalignment='right',size=6)\n",
    "plt.ylabel('Number of Pictures')\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the first image in each class. Enter 0 to 42.\n",
    "\n",
    "\n",
    "plt.imshow(X_train[np.sum(number_sign[0:35])+50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "----\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\n",
    "# Not used in final classifier, full color image is used instead.\n",
    "def grayscale(img):\n",
    "    #call plt.imshow(gray, cmap='gray')\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "X_train_gray = np.zeros( (len(X_train), len(X_train[0]), len(X_train[0,0]), 1 ), dtype='uint8' )\n",
    "X_test_gray = np.zeros( (len(X_test), len(X_test[0]), len(X_test[0,0]) , 1 ), dtype='uint8' )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range( len(X_train) ):\n",
    "    X_train_gray[i,:,:,0] = grayscale(X_train[i])\n",
    "\n",
    "for i in range( len(X_test) ):\n",
    "    X_test_gray[i,:,:,0] = grayscale(X_test[i])\n",
    "    \n",
    "\n",
    "print(X_test_gray[0,0,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Normalize data, use a and b values from lab.\n",
    "\n",
    "def normalize_image(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # ToDo: Implement Min-Max scaling for greyscale image data\n",
    "    grey_max = 255\n",
    "    grey_min = 0\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    \n",
    "    norm_image_data = a + ( (image_data - grey_min)*(b - a)/(grey_max - grey_min) )\n",
    "    \n",
    "    return norm_image_data\n",
    "\n",
    "X_train_norm = np.zeros( (len(X_train), len(X_train[0]), len(X_train[0,0]), 3 ), dtype='float32' )\n",
    "X_test_norm = np.zeros( (len(X_test), len(X_test[0]), len(X_test[0,0]), 3 ), dtype='float32' )\n",
    "\n",
    "for i in range( len(X_train) ):\n",
    "    X_train_norm[i] = normalize_image(X_train[i])\n",
    "    #X_train_norm[i] = normalize_image(X_train_gray[i])\n",
    "\n",
    "for i in range( len(X_test) ):\n",
    "    X_test_norm[i] = normalize_image(X_test[i])\n",
    "\n",
    "print( X_train[0,0,:,2] )\n",
    "print( X_train_norm[0,0,:,2] )\n",
    "print( '\\n')\n",
    "print( X_test[0,0,:,2] )\n",
    "print( X_test_norm[0,0,:,2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## One Hot Encoding of labels\n",
    "\n",
    "# Test\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "testlabel = np.arange(10)\n",
    "testlabel = np.append(testlabel,(5,1,2))\n",
    "encoder.fit(testlabel)\n",
    "testlabel = encoder.transform(testlabel)\n",
    "\n",
    "print(testlabel)\n",
    "print('\\n')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit( y_train )\n",
    "y_train = encoder.transform( y_train )\n",
    "\n",
    "y_train = y_train.astype(dtype='float32')\n",
    "\n",
    "print( y_train )\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit( y_test )\n",
    "y_test = encoder.transform( y_test )\n",
    "\n",
    "y_test = y_test.astype(dtype='float32')\n",
    "print( '\\n' )\n",
    "print( y_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Generate data additional (if you want to!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    X_train_norm,\n",
    "    y_train,\n",
    "    test_size=0,\n",
    "    random_state=1336)\n",
    "\n",
    "print( str(len(y_train)) + ' - '+ str(len(train_labels)) + ' = ' + str(len(valid_labels)))\n",
    "\n",
    "print(train_features[0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'preprocdata.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('preprocdata.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': X_test_norm,  \n",
    "                    'test_labels': y_test,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(X_test_norm))\n",
    "print(len(train_features))\n",
    "print(len(valid_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test importing all required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Modules loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "pickle_file = 'preprocdata.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels'] \n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "    \n",
    "print('Data and modules loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to ensure data is displaying correctly.\n",
    "\n",
    "print(train_features[0,0,0,:])\n",
    "#print(valid_features[0,0,0,:])\n",
    "print(test_features[0,0,0,:])\n",
    "\n",
    "print(train_labels[:,0])\n",
    "#print(valid_labels[:,0])\n",
    "print(test_labels[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolution kernal final size.\n",
    "kernal_size = 3\n",
    "\n",
    "# Convolutional filters.\n",
    "nb_filters = 16\n",
    "pool_size = (2,2)\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First convolution layer with relu activation, max pooling, and dropout.\n",
    "model.add(Convolution2D(nb_filters, kernal_size*2, kernal_size*2,\n",
    "                        border_mode = 'same',\n",
    "                        input_shape = input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second convolution layer with relu, max pooling, and dropout.\n",
    "model.add(Convolution2D(nb_filters*2, kernal_size, kernal_size,\n",
    "                        border_mode = 'same',\n",
    "                        input_shape = input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Third convolution layer with relu, max pooling, and dropout.\n",
    "model.add(Convolution2D(nb_filters*4, kernal_size, kernal_size,\n",
    "                        border_mode = 'same',\n",
    "                        input_shape = input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# First fully connected deep layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Final dense layer.\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 15\n",
    "\n",
    "prevepoch = 5\n",
    "print('Before this there was', prevepoch, 'epochs.')\n",
    "\n",
    "batch_size = 50 \n",
    "\n",
    "starttime = time.clock()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    batch_size = batch_size, nb_epoch = nb_epoch,\n",
    "                    verbose = 1, validation_split = 0.15)\n",
    "\n",
    "elapsedtime = time.clock() - starttime\n",
    "\n",
    "print('%.1f minutes.\\n\\n' %(elapsedtime/60))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test accuracy.\n",
    "testacc = model.evaluate(test_features, test_labels)\n",
    "\n",
    "print(testacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Tensor Flow Code\n",
    "---\n",
    "\n",
    "The code below was written using tensor flow to build an image classifier. I have replaced this code with a better keras classifier. I'm keeping this old code here as I might use it in the future as a reference if if I ever use tensor flow without keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Code modified from \"Deep MNIST for Experts\" tutorial which had already developed the type of network\n",
    "# I was looking to build which was: a few alternating convolution and pooling layers capped off by a few densely\n",
    "# connected layers.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Convolution Layer 1 with pooling\n",
    "#x = tf.placeholder(tf.float32, [None, 32*32])\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32,3])\n",
    "y_ = tf.placeholder(tf.float32, [None, 43])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 32]) # 32 features for each 5x5 patch\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "#x_image = tf.reshape(x, [-1,32,32,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu( conv2d(x, W_conv1) + b_conv1) # First layer output.\n",
    "h_pool1 = max_pool_2x2(h_conv1) # apply max pooling after each conv layer.\n",
    "\n",
    "\n",
    "# Convolution Layer 2 with pooling\n",
    "W_conv2 = weight_variable([5, 5, 32, 64]) # 5x5 patch, 64 features.\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu( conv2d(h_pool1, W_conv2) + b_conv2 )\n",
    "h_pool2 = max_pool_2x2( h_conv2 ) \n",
    "\n",
    "# Because the image has been pooled at 2x2 twice, the image size is now 32/(2*2) = 8. Image size is 8x8x3.\n",
    "# Densely Connected Layer 3 with 1024 neurons.\n",
    "W_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape( h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu( tf.matmul( h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "# Dropout to increase redundancy and reduce overfitting.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout( h_fc1, keep_prob)\n",
    "\n",
    "# Softmax Layer 4 to output probabilities. Output must be 43. \n",
    "W_fc2 = weight_variable([1024, 43])\n",
    "b_fc2 = bias_variable([43])\n",
    "\n",
    "y_conv = tf.nn.softmax( tf.matmul( h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "print('Got here.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "### Model Training and Evaluating\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "# Feed dicts for training, validation, and test session\n",
    "train_feed_dict = {x: train_features, y_: train_labels, keep_prob: 1.0}\n",
    "valid_feed_dict = {x: valid_features, y_: valid_labels, keep_prob: 1.0}\n",
    "test_feed_dict = {x: test_features, y_: test_labels, keep_prob: 1.0}\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal( tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "batch_count = int(math.ceil(len(train_features)/batch_size)) \n",
    "for epoch_i in range(epochs):\n",
    "    # Progress bar\n",
    "    batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "    # The training cycle\n",
    "    for batch_i in batches_pbar:\n",
    "        # Get a batch of training features and labels\n",
    "        batch_start = batch_i*batch_size\n",
    "        batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "        batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        #if batch_i % 30000 == 0:\n",
    "            #valid_accuracy = accuracy.eval(valid_feed_dict)\n",
    "            #print(\"\\n Validation accuracy %g\" %(valid_accuracy))\n",
    "            \n",
    "        \n",
    "        train_step.run(feed_dict = {x: batch_features, y_: batch_labels, keep_prob: 0.5})\n",
    "    \n",
    "    valid_accuracy = accuracy.eval(valid_feed_dict)\n",
    "    print(\"\\n Validation accuracy %g\" %(valid_accuracy))\n",
    "    #print(\"Test accuracy %g\"%accuracy.eval(test_feed_dict))\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "save_path = saver.save(sess, \"color-model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Valid accuracy %g\"%accuracy.eval(valid_feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Test accuracy %g\"%accuracy.eval(test_feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Train accuracy %g\"%accuracy.eval(train_feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imnum = 380\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"31e-model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #print(y_conv)\n",
    "    #print(\"Valid accuracy %g\"%accuracy.eval(valid_feed_dict))\n",
    "    #print(\"Test accuracy %g\"%accuracy.eval(test_feed_dict))\n",
    "    \n",
    "    feed_dict = {x: X_test_norm[imnum:imnum+10], keep_prob: 1.0}\n",
    "    classificationout = sess.run(y_conv, feed_dict)\n",
    "    #print( classificationout[0] )\n",
    "        \n",
    "    predictionout=tf.argmax(y_conv,1)\n",
    "    prednum = predictionout.eval(feed_dict={x: X_test_norm[imnum:imnum+10], keep_prob: 1.0})\n",
    "    \n",
    "    #print(prednum)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    for i in range(0, 10):\n",
    "        guessprob = classificationout[i,prednum[i]]\n",
    "        print('Guess Probability: ' + str(guessprob) + ' %')\n",
    "\n",
    "        print('Sign Name: ' + signnameslist[prednum[i]])\n",
    "    \n",
    "        #correct_prediction = tf.equal( tf.argmax(y_conv,1), tf.argmax(y_test[imnum:imnum+1],1))\n",
    "        #accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )\n",
    "        #print(accuracy.eval(feed_dict={x: X_test_norm[imnum:imnum+1], y_: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "        plt.imshow(X_test[imnum+i])\n",
    "        plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\n",
    "# Load images\n",
    "self_img = np.zeros((5,32,32,3), dtype='uint8')\n",
    "\n",
    "self_img[0,:,:,:] = mpimg.imread('photos/0.jpg')\n",
    "self_img[1,:,:,:] = mpimg.imread('photos/2.jpg')\n",
    "self_img[2,:,:,:] = mpimg.imread('photos/3.jpg')\n",
    "self_img[3,:,:,:] = mpimg.imread('photos/4.jpg')\n",
    "self_img[4,:,:,:] = mpimg.imread('photos/5.jpg')\n",
    "\n",
    "self_img_gray = np.zeros((5,32,32,1), dtype='uint8')\n",
    "\n",
    "# Grayscale\n",
    "for i in range( len(self_img) ):\n",
    "    self_img_gray[i,:,:,0] = grayscale(self_img[i])\n",
    "    \n",
    "self_img_norm = np.zeros((5,32,32,1), dtype='float32')\n",
    "    \n",
    "# Normalize\n",
    "for i in range( len(self_img_gray) ):\n",
    "    self_img_norm[i] = normalize_greyscale(self_img_gray[i])\n",
    "    \n",
    "print(self_img_gray[0,:,0,0])\n",
    "print(self_img_norm[0,:,0,0])\n",
    "\n",
    "\n",
    "print('Preprocess done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "imnum = 0\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"31e-model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    #print(y_conv)\n",
    "    #print(\"Valid accuracy %g\"%accuracy.eval(valid_feed_dict))\n",
    "    #print(\"Test accuracy %g\"%accuracy.eval(test_feed_dict))\n",
    "    \n",
    "    feed_dict = {x: self_img_norm[imnum:imnum+5], keep_prob: 1.0}\n",
    "    classificationout = sess.run(y_conv, feed_dict)\n",
    "    #print( classificationout[0] )\n",
    "        \n",
    "    predictionout=tf.argmax(y_conv,1)\n",
    "    prednum = predictionout.eval(feed_dict={x: self_img_norm[imnum:imnum+5], keep_prob: 1.0})\n",
    "    \n",
    "    print(prednum)\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        guessprob = classificationout[i,prednum[i]]\n",
    "        print('Guess Probability: ' + str(guessprob*100) + ' %')\n",
    "\n",
    "        print('Guess Sign Name: ' + signnameslist[prednum[i]])\n",
    "    \n",
    "        #correct_prediction = tf.equal( tf.argmax(y_conv,1), tf.argmax(y_test[imnum:imnum+1],1))\n",
    "        #accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )\n",
    "        #print(accuracy.eval(feed_dict={x: X_test_norm[imnum:imnum+1], y_: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "        plt.imshow(self_img[imnum+i])\n",
    "        plt.show()\n",
    "        \n",
    "    guessprob = classificationout[4,2]\n",
    "    print('Correct Probability: ' + str(guessprob*100) + ' %')\n",
    "    guessprob = classificationout[4,:]\n",
    "    print('All Probabilities:\\n')\n",
    "    print(guessprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualization of Softmax Probabilities\n",
    "import heapq\n",
    "\n",
    "values = heapq.nlargest(5, classificationout[4,:])\n",
    "stopsignname = [signnameslist[35], signnameslist[3], signnameslist[1], signnameslist[25], signnameslist[0]]\n",
    "plt.bar(np.arange(0,5),values, alpha=.99)\n",
    "plt.xticks(np.arange(0.5,5.5), stopsignname, rotation=30, horizontalalignment='right',size=10)\n",
    "plt.ylabel('Guess Probability %')\n",
    "plt.title('50 km/h Softmax Probability')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "values = heapq.nlargest(5, classificationout[0,:])\n",
    "stopsignname = [signnameslist[14], signnameslist[36], signnameslist[17], signnameslist[22], signnameslist[38]]\n",
    "                \n",
    "plt.bar(np.arange(0,5),values, alpha=.66)\n",
    "plt.xticks(np.arange(0.5,5.5), stopsignname, rotation=30, horizontalalignment='right',size=10)\n",
    "plt.ylabel('Guess Probability %')\n",
    "plt.title('Stop Sign Softmax Probability')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "values = heapq.nlargest(5, classificationout[2,:])\n",
    "stopsignname = [signnameslist[25], signnameslist[35], signnameslist[13], signnameslist[33], signnameslist[10]]\n",
    "plt.bar(np.arange(0,5),values, alpha=0.33)\n",
    "plt.xticks(np.arange(0.5,5.5), stopsignname, rotation=30, horizontalalignment='right',size=10)\n",
    "plt.ylabel('Guess Probability %')\n",
    "plt.title('Road Work Softmax Probability')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
